
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>bayesflow.trainers module &#8212; BayesFlow: Amortized Bayesian Inference</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8fec244e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/bayesflow.trainers';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://www.bayesflow.org/api/bayesflow.trainers.html" />
    <link rel="icon" href="../_static/bayesflow_hex.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="bayesflow.configuration module" href="bayesflow.configuration.html" />
    <link rel="prev" title="bayesflow.summary_networks module" href="bayesflow.summary_networks.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/bayesflow_hex.png" class="logo__image only-light" alt="BayesFlow: Amortized Bayesian Inference - Home"/>
    <img src="../_static/bayesflow_hex.png" class="logo__image only-dark pst-js-only" alt="BayesFlow: Amortized Bayesian Inference - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../index.html">BayesFlow</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Intro_Amortized_Posterior_Estimation.html">1. Quickstart: Amortized Posterior Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/TwoMoons_Bimodal_Posterior.html">2. Two Moons: Tackling Bimodal Posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Model_Misspecification.html">3. Detecting Model Misspecification in Amortized Posterior Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/LCA_Model_Posterior_Estimation.html">4. Principled Amortized Bayesian Workflow for Cognitive Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Linear_ODE_system.html">5. Posterior Estimation for ODEs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Covid19_Initial_Posterior_Estimation.html">6. Posterior Estimation for SIR-like Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Model_Comparison_MPT.html">7. Model Comparison for Cognitive Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_examples/Hierarchical_Model_Comparison_MPT.html">8. Hierarchical Model Comparison for Cognitive Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="bayesflow.html">Public API: bayesflow package</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="bayesflow.benchmarks.html">bayesflow.benchmarks package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.bernoulli_glm.html">bayesflow.benchmarks.bernoulli_glm module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.bernoulli_glm_raw.html">bayesflow.benchmarks.bernoulli_glm_raw module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.gaussian_linear.html">bayesflow.benchmarks.gaussian_linear module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.gaussian_linear_uniform.html">bayesflow.benchmarks.gaussian_linear_uniform module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.gaussian_mixture.html">bayesflow.benchmarks.gaussian_mixture module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.inverse_kinematics.html">bayesflow.benchmarks.inverse_kinematics module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.lotka_volterra.html">bayesflow.benchmarks.lotka_volterra module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.sir.html">bayesflow.benchmarks.sir module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.slcp.html">bayesflow.benchmarks.slcp module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.slcp_distractors.html">bayesflow.benchmarks.slcp_distractors module</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.benchmarks.two_moons.html">bayesflow.benchmarks.two_moons module</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.amortizers.html">bayesflow.amortizers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.attention.html">bayesflow.attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.coupling_networks.html">bayesflow.coupling_networks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.diagnostics.html">bayesflow.diagnostics module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.inference_networks.html">bayesflow.inference_networks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.losses.html">bayesflow.losses module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.networks.html">bayesflow.networks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.sensitivity.html">bayesflow.sensitivity module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.simulation.html">bayesflow.simulation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.summary_networks.html">bayesflow.summary_networks module</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">bayesflow.trainers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.configuration.html">bayesflow.configuration module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.default_settings.html">bayesflow.default_settings module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.computational_utilities.html">bayesflow.computational_utilities module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.helper_classes.html">bayesflow.helper_classes module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.helper_functions.html">bayesflow.helper_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.helper_networks.html">bayesflow.helper_networks module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.exceptions.html">bayesflow.exceptions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.mcmc.html">bayesflow.mcmc module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.version.html">bayesflow.version module</a></li>
<li class="toctree-l2"><a class="reference internal" href="bayesflow.wrappers.html">bayesflow.wrappers module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="bayesflow.experimental.html">bayesflow.experimental package</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="bayesflow.experimental.rectifiers.html">bayesflow.experimental.rectifiers module</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Full Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About us</a></li>
</ul>

    </div>
</nav></div>
        <div class="sidebar-primary-item">
<div class="rst-versions">
   
  <p class="caption" aria-level="2" role="heading"><span class="caption-text">Tags</span></p>
  <ul>
      <li><a href="/v1.1.6/index.html" >v1.1.6</a></li>
      <li><a href="/v1.1.5/index.html" >v1.1.5</a></li>
      <li><a href="/v1.1.4/index.html" >v1.1.4</a></li>
      <li><a href="/v1.1.3/index.html" >v1.1.3</a></li>
      <li><a href="/v1.1.2/index.html" >v1.1.2</a></li>
  </ul>
  
   
  <p class="caption" aria-level="2" role="heading"><span class="caption-text">Branches</span></p>
  <ul>
      <li><a href="/master/index.html" class="current">master</a></li>
  </ul>
  
</div>
</div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bayesflow-org/bayesflow" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bayesflow-org/bayesflow/edit/master/api/bayesflow.trainers.rst" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bayesflow-org/bayesflow/issues/new?title=Issue%20on%20page%20%2Fapi/bayesflow.trainers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/api/bayesflow.trainers.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>bayesflow.trainers module</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer"><code class="docutils literal notranslate"><span class="pre">Trainer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.__init__"><code class="docutils literal notranslate"><span class="pre">Trainer.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.diagnose_latent2d"><code class="docutils literal notranslate"><span class="pre">Trainer.diagnose_latent2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.diagnose_sbc_histograms"><code class="docutils literal notranslate"><span class="pre">Trainer.diagnose_sbc_histograms()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.load_pretrained_network"><code class="docutils literal notranslate"><span class="pre">Trainer.load_pretrained_network()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_online"><code class="docutils literal notranslate"><span class="pre">Trainer.train_online()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_offline"><code class="docutils literal notranslate"><span class="pre">Trainer.train_offline()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_from_presimulation"><code class="docutils literal notranslate"><span class="pre">Trainer.train_from_presimulation()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_experience_replay"><code class="docutils literal notranslate"><span class="pre">Trainer.train_experience_replay()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_rounds"><code class="docutils literal notranslate"><span class="pre">Trainer.train_rounds()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.mmd_hypothesis_test"><code class="docutils literal notranslate"><span class="pre">Trainer.mmd_hypothesis_test()</span></code></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-bayesflow.trainers">
<span id="bayesflow-trainers-module"></span><h1>bayesflow.trainers module<a class="headerlink" href="#module-bayesflow.trainers" title="Link to this heading">#</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesflow.trainers.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">amortizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generative_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configurator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_checks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class connects a generative model (or, already simulated data from a model) with
a configurator and a neural inference architecture for amortized inference (amortizer). A Trainer
instance is responsible for optimizing the amortizer via various forms of simulation-based training.</p>
<p>At the very minimum, the trainer must be initialized with an <cite>amortizer</cite> instance, which is capable
of processing the (configured) outputs of a generative model. A <cite>configurator</cite> will then process
the outputs of the generative model and convert them into suitable inputs for the amortizer. Users
can choose from a palette of default configurators or create their own configurators, essentially
building a modularized pipeline <cite>GenerativeModel</cite> -&gt; <cite>Configurator</cite> -&gt; <cite>Amortizer</cite>. Most complex models
will require custom configurators.</p>
<p class="rubric">Notes</p>
<p>Currently, the trainer supports the following simulation-based training regimes, based on efficiency
considerations:</p>
<ul>
<li><p>Online training</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train_online</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">iterations_per_epoch</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>This training regime is optimal for fast generative models which can efficiently simulated data on-the-fly.
In order for this training regime to be efficient, on-the-fly batch simulations should not take longer
than 2-3 seconds.</p>
</li>
<li><p>Experience replay training</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train_experience_replay</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">iterations_per_epoch</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>This training regime is also good for fast generative models capable of efficiently simulating data on-the-fly.
Compare to pure online training, this training will keep an experience replay buffer from which simulations
are randomly sampled, so the networks will likely see some simulations multiple times.</p>
</li>
<li><p>Round-based training</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train_rounds</span><span class="p">(</span><span class="n">rounds</span><span class="p">,</span> <span class="n">sim_per_round</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>This training regime is optimal for slow, but still reasonably performant generative models.
In order for this training regime to be efficient, on-the-fly batch simulations should not take
longer than 2-3 minutes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>overfitting presents a danger when using small numbers of simulated data sets, so it is recommended
to use some amount of regularization for the neural amortizer(s).</p>
</div>
</li>
<li><p>Offline training</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span><span class="o">.</span><span class="n">train_offline</span><span class="p">(</span><span class="n">simulations_dict</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>This training regime is optimal for very slow, external simulators, which take several minutes for a
single simulation. It assumes that all training data has been already simulated and stored on disk.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Overfitting presents a danger when using a small simulated data set, so it is recommended to use
some amount of regularization for the neural amortizer(s).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For extremely slow simulators (i.e., more than an hour of a single simulation), the BayesFlow framework
might not be the ideal choice and should probably be considered in combination with a black-box surrogate
optimization method, such as Bayesian optimization.</p>
</div>
</li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">amortizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generative_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configurator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_to_keep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_checks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.__init__" title="Link to this definition">#</a></dt>
<dd><p>Creates a trainer which will use a generative model (or data simulated from it) to optimize
a neural architecture (amortizer) for amortized posterior inference, likelihood inference, or both.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>amortizer</strong><span class="classifier"><cite>bayesflow.amortizers.Amortizer</cite></span></dt><dd><p>The neural architecture to be optimized.</p>
</dd>
<dt><strong>generative_model</strong><span class="classifier"><cite>bayesflow.forward_inference.GenerativeModel</cite></span></dt><dd><p>A generative model returning a dictionary with randomly sampled parameters, data, and optional context</p>
</dd>
<dt><strong>configurator</strong><span class="classifier">callable or None, optional, default: None</span></dt><dd><p>A callable object transforming and combining the outputs of the generative model into inputs for a BayesFlow
amortizer.</p>
</dd>
<dt><strong>checkpoint_path</strong><span class="classifier">string or None, optional, default: None</span></dt><dd><p>Optional file path for storing the trained amortizer, loss history and optional memory.</p>
</dd>
<dt><strong>max_to_keep</strong><span class="classifier">int, optional, default: 3</span></dt><dd><p>Number of checkpoints and loss history snapshots to keep.</p>
</dd>
<dt><strong>default_lr</strong><span class="classifier">float, optional, default: 0.0005</span></dt><dd><p>The default learning rate to use for default optimizers.</p>
</dd>
<dt><strong>skip_checks</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>If True, do not perform consistency checks, i.e., simulator runs and passed through nets</p>
</dd>
<dt><strong>memory</strong><span class="classifier">bool or bayesflow.SimulationMemory, optional, default: False</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, store a pre-defined amount of simulations for later use (validation, etc.).
If <cite>SimulationMemory</cite> instance provided, stores a reference to the instance.
Otherwise the corresponding attribute will be set to None.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.diagnose_latent2d">
<span class="sig-name descname"><span class="pre">diagnose_latent2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.diagnose_latent2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.diagnose_latent2d" title="Link to this definition">#</a></dt>
<dd><p>Performs visual pre-inference diagnostics of latent space on either provided validation data
(new simulations) or internal simulation memory.
If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, then diagnostics will be performed on the inputs, regardless
whether the <cite>simulation_memory</cite> of the trainer is empty or not. If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">None</span></code>, then
the trainer will try to access is memory or raise a <cite>ConfigurationError</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>inputs</strong><span class="classifier">None, list, or dict, optional, default: None</span></dt><dd><p>The optional inputs to use</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>fig</strong><span class="classifier">plt.Figure</span></dt><dd><p>The figure object which can be readily saved to disk using <cite>fig.savefig()</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>conf_args</strong></dt><dd><p>optional keyword arguments passed to the configurator</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional keyword arguments passed to the amortizer</p>
</dd>
<dt><strong>plot_args</strong></dt><dd><p>optional keyword arguments passed to <cite>plot_latent_space_2d</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.diagnose_sbc_histograms">
<span class="sig-name descname"><span class="pre">diagnose_sbc_histograms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.diagnose_sbc_histograms"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.diagnose_sbc_histograms" title="Link to this definition">#</a></dt>
<dd><p>Performs visual pre-inference diagnostics via simulation-based calibration (SBC)
(new simulations) or internal simulation memory.
If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, then diagnostics will be performed on the inputs, regardless
whether the <cite>simulation_memory</cite> of the trainer is empty or not. If <code class="docutils literal notranslate"><span class="pre">inputs</span> <span class="pre">is</span> <span class="pre">None</span></code>, then
the trainer will try to access is memory or raise a <cite>ConfigurationError</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>inputs</strong><span class="classifier">None, list or dict, optional, default: None</span></dt><dd><p>The optional inputs to use</p>
</dd>
<dt><strong>n_samples</strong><span class="classifier">int or None, optional, default: None</span></dt><dd><p>The number of posterior samples to draw for each simulated data set.
If None, the number will be heuristically determined so that n_sim / n_draws is approximately equal to 20</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>fig</strong><span class="classifier">plt.Figure</span></dt><dd><p>The figure object which can be readily saved to disk using <cite>fig.savefig()</cite>.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>conf_args</strong></dt><dd><p>optional keyword arguments passed to the configurator</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional keyword arguments passed to the amortizer</p>
</dd>
<dt><strong>plot_args</strong></dt><dd><p>optional keyword arguments passed to <cite>plot_sbc()</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.load_pretrained_network">
<span class="sig-name descname"><span class="pre">load_pretrained_network</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.load_pretrained_network"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.load_pretrained_network" title="Link to this definition">#</a></dt>
<dd><p>Attempts to load a pre-trained network if checkpoint path is provided and a checkpoint manager exists.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_online">
<span class="sig-name descname"><span class="pre">train_online</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations_per_epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.train_online"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_online" title="Link to this definition">#</a></dt>
<dd><p>Trains an amortizer via online learning. Additional keyword arguments
are passed to the generative mode, configurator, and amortizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epochs</strong><span class="classifier">int</span></dt><dd><p>Number of epochs (and number of times a checkpoint is stored)</p>
</dd>
<dt><strong>iterations_per_epoch</strong><span class="classifier">int</span></dt><dd><p>Number of batch simulations to perform per epoch</p>
</dd>
<dt><strong>batch_size</strong><span class="classifier">int</span></dt><dd><p>Number of simulations to perform at each backprop step</p>
</dd>
<dt><strong>save_checkpoint</strong><span class="classifier">bool, default: True</span></dt><dd><p>A flag to decide whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p>
</dd>
<dt><strong>optimizer</strong><span class="classifier">tf.keras.optimizer.Optimizer or None</span></dt><dd><p>Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p>
</dd>
<dt><strong>reuse_optimizer</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <a href="#id1"><span class="problematic" id="id2">``</span></a>self.optimizer` and re-used in further training runs.</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>Whether to use optional stopping or not during training. Could speed up training.
Only works if <code class="docutils literal notranslate"><span class="pre">validation_sims</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, i.e., validation data has been provided.</p>
</dd>
<dt><strong>use_autograph</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use autograph for the backprop step. Could lead to enormous speed-ups but
could also be harder to debug.</p>
</dd>
<dt><strong>validation_sims</strong><span class="classifier">dict or None, optional, default: None</span></dt><dd><p>Simulations used as a “validation set”.
If <code class="docutils literal notranslate"><span class="pre">dict</span></code>, will assume it’s the output of a generative model and try
<code class="docutils literal notranslate"><span class="pre">amortizer.compute_loss(configurator(validation_sims))</span></code>
after each epoch.
If <code class="docutils literal notranslate"><span class="pre">int</span></code>, will assume it’s the number of sims to generate from the generative
model before starting training. Only considered if a generative model has been
provided during initialization.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), no validation set will be used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>losses</strong><span class="classifier">dict or pandas.DataFrame</span></dt><dd><p>A dictionary storing the losses across epochs and iterations</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model_args</strong></dt><dd><p>optional kwargs passed to the generative model</p>
</dd>
<dt><strong>val_model_args:</strong></dt><dd><p>optional kwargs passed to the generative model for generating validation data. Only useful if
<code class="docutils literal notranslate"><span class="pre">type(validation_sims)</span> <span class="pre">is</span> <span class="pre">int</span></code>.</p>
</dd>
<dt><strong>conf_args</strong></dt><dd><p>optional kwargs passed to the configurator before each backprop (update) step.</p>
</dd>
<dt><strong>val_conf_args</strong></dt><dd><p>optional kwargs passed to the configurator then configuring the validation data.</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional kwargs passed to the amortizer</p>
</dd>
<dt><strong>early_stopping_args</strong></dt><dd><p>optional kwargs passed to the <cite>EarlyStopper</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_offline">
<span class="sig-name descname"><span class="pre">train_offline</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">simulations_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.train_offline"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_offline" title="Link to this definition">#</a></dt>
<dd><p>Trains an amortizer via offline learning. Assume parameters, data and optional
context have already been simulated (i.e., forward inference has been performed).</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>simulations_dict</strong><span class="classifier">dict</span></dt><dd><p>A dictionary containing the simulated data / context, if using the default keys,
the method expects at least the mandatory keys <code class="docutils literal notranslate"><span class="pre">sim_data</span></code> and <code class="docutils literal notranslate"><span class="pre">prior_draws</span></code> to be present</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int</span></dt><dd><p>Number of epochs (and number of times a checkpoint is stored)</p>
</dd>
<dt><strong>batch_size</strong><span class="classifier">int</span></dt><dd><p>Number of simulations to perform at each backpropagation step</p>
</dd>
<dt><strong>save_checkpoint</strong><span class="classifier">bool, default: True</span></dt><dd><p>Determines whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p>
</dd>
<dt><strong>optimizer</strong><span class="classifier">tf.keras.optimizer.Optimizer or None</span></dt><dd><p>Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p>
</dd>
<dt><strong>reuse_optimizer</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>Whether to use optional stopping or not during training. Could speed up training.
Only works if <code class="docutils literal notranslate"><span class="pre">validation_sims</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, i.e., validation data has been provided.</p>
</dd>
<dt><strong>use_autograph</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use autograph for the backprop step. Could lead to enormous speed-ups but
could also be harder to debug.</p>
</dd>
<dt><strong>validation_sims</strong><span class="classifier">dict, int, or None, optional, default: None</span></dt><dd><p>Simulations used as a “validation set”.
If <code class="docutils literal notranslate"><span class="pre">dict</span></code>, will assume it’s the output of a generative model and try
<code class="docutils literal notranslate"><span class="pre">amortizer.compute_loss(configurator(validation_sims))</span></code> after each epoch.
If <code class="docutils literal notranslate"><span class="pre">int</span></code>, will assume it’s the number of sims to generate from the generative
model before starting training. Only considered if a generative model has been
provided during initialization.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), no validation set will be used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>losses</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></span></dt><dd><p>A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>val_model_args</strong></dt><dd><p>optional kwargs passed to the generative model for generating validation data.
Only useful if <code class="docutils literal notranslate"><span class="pre">type(validation_sims)</span> <span class="pre">is</span> <span class="pre">int</span></code>.</p>
</dd>
<dt><strong>conf_args</strong></dt><dd><p>optional kwargs passed to the configurator before each backprop (update) step.</p>
</dd>
<dt><strong>val_conf_args</strong></dt><dd><p>optional kwargs passed to the configurator then configuring the validation data.</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional kwargs passed to the amortizer</p>
</dd>
<dt><strong>early_stopping_args</strong></dt><dd><p>optional kwargs passed to the <cite>EarlyStopper</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_from_presimulation">
<span class="sig-name descname"><span class="pre">train_from_presimulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">presimulation_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.train_from_presimulation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_from_presimulation" title="Link to this definition">#</a></dt>
<dd><p>Trains an amortizer via a modified form of offline training.</p>
<p>Like regular offline training, it assumes that parameters, data and optional context have already
been simulated (i.e., forward inference has been performed).</p>
<p>Also like regular offline training, it is faster than online training in scenarios where simulations are slow.
Unlike regular offline training, it uses each batch from the presimulated dataset only once during training,
if not otherwise specified by a higher maximal number of epochs. Then, presimulated data is reused in a cyclic
manner to achieve the desired number of epochs.
A larger presimulated dataset is therefore required than for offline training, and the increase in speed
gained by loading simulations instead of generating them on the fly comes at a cost:
a large presimulated dataset takes up a large amount of hard drive space.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>presimulation_path</strong><span class="classifier">str</span></dt><dd><p>File path to the folder containing the files from the precomputed simulation.
Ideally generated using a GenerativeModel’s presimulate_and_save method, otherwise must match
the structure produced by that method.
Each file contains the data for one epoch (i.e. a number of batches), and must be compatible
with the custom_loader provided.
The custom_loader must read each file into a collection (either a dictionary or a list) of simulation_dict
objects.
This is easily achieved with the pickle library: if the files were generated from collections of
simulation_dict objects using pickle.dump, the _default_loader (default for custom_load) will
load them using pickle.load.
Training parameters like number of iterations and batch size are inferred from the files during training.</p>
</dd>
<dt><strong>optimizer</strong><span class="classifier">tf.keras.optimizer.Optimizer</span></dt><dd><p>Optimizer for the neural network training. Since for this training, it is impossible to guess the number of
iterations beforehead, an optimizer must be provided.</p>
</dd>
<dt><strong>save_checkpoint</strong><span class="classifier">bool, optional, default</span><span class="classifier">True</span></dt><dd><p>Determines whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p>
</dd>
<dt><strong>max_epochs</strong><span class="classifier">int or None, optional, default: None</span></dt><dd><p>An optional parameter to limit or extend the number of epochs. If number of epochs is larger than the files
of the dataset, presimulations will be reused.</p>
</dd>
<dt><strong>reuse_optimizer</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p>
</dd>
<dt><strong>custom_loader</strong><span class="classifier">callable, optional, default: self._default_loader</span></dt><dd><p>Must take a string file_path as an input and output a collection (dictionary or list) of
simulation_dict objects. A simulation_dict has the keys <code class="docutils literal notranslate"><span class="pre">prior_non_batchable_context</span></code>,
<code class="docutils literal notranslate"><span class="pre">prior_batchable_context</span></code>, <code class="docutils literal notranslate"><span class="pre">prior_draws</span></code>, <code class="docutils literal notranslate"><span class="pre">sim_non_batchable_context</span></code>, <code class="docutils literal notranslate"><span class="pre">sim_batchable_context</span></code>, and
<code class="docutils literal notranslate"><span class="pre">sim_data</span></code>.
Here, <code class="docutils literal notranslate"><span class="pre">prior_draws</span></code> and <code class="docutils literal notranslate"><span class="pre">sim_data</span></code> must have actual data as values, the rest are optional.</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>Whether to use optional stopping or not during training. Could speed up training.</p>
</dd>
<dt><strong>validation_sims</strong><span class="classifier">dict, int, or None, optional, default: None</span></dt><dd><p>Simulations used as a validation set.
If <code class="docutils literal notranslate"><span class="pre">dict</span></code>, will assume it’s the output of a generative model and try
<code class="docutils literal notranslate"><span class="pre">amortizer.compute_loss(configurator(validation_sims))</span></code>
after each epoch.
If <code class="docutils literal notranslate"><span class="pre">int</span></code>, will assume it’s the number of sims to generate from the generative
model before starting training. Only considered if a generative model has been
provided during initialization.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), no validation set will be used.</p>
</dd>
<dt><strong>use_autograph</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use autograph for the backprop step. Could lead to enormous speed-ups but
could also be harder to debug.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>losses</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></span></dt><dd><p>A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>conf_args</strong></dt><dd><p>optional keyword arguments passed to the configurator</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional keyword arguments passed to the amortizer</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_experience_replay">
<span class="sig-name descname"><span class="pre">train_experience_replay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations_per_epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer_capacity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.train_experience_replay"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_experience_replay" title="Link to this definition">#</a></dt>
<dd><p>Trains the network(s) via experience replay using a memory replay buffer, as utilized
in reinforcement learning. Additional keyword arguments are passed to the generative mode,
configurator, and amortizer. Read below for signature.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>epochs</strong><span class="classifier">int</span></dt><dd><p>Number of epochs (and number of times a checkpoint is stored)</p>
</dd>
<dt><strong>iterations_per_epoch</strong><span class="classifier">int</span></dt><dd><p>Number of batch simulations to perform per epoch</p>
</dd>
<dt><strong>batch_size</strong><span class="classifier">int</span></dt><dd><p>Number of simulations to perform at each backpropagation step.</p>
</dd>
<dt><strong>save_checkpoint</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>A flag to decide whether to save checkpoints after each epoch,
if a <code class="docutils literal notranslate"><span class="pre">checkpoint_path</span></code> provided during initialization, otherwise ignored.</p>
</dd>
<dt><strong>optimizer</strong><span class="classifier">tf.keras.optimizer.Optimizer or None</span></dt><dd><p>Optimizer for the neural network. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p>
</dd>
<dt><strong>reuse_optimizer</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p>
</dd>
<dt><strong>buffer_capacity</strong><span class="classifier">int, optional, default: 1000</span></dt><dd><p>Max number of batches to store in buffer. For instance, if <code class="docutils literal notranslate"><span class="pre">batch_size=32</span></code>
and <code class="docutils literal notranslate"><span class="pre">capacity_in_batches=1000</span></code>, then the buffer will hold a maximum of
32 * 1000 = 32000 simulations. Be careful with memory!
Important! Argument will be ignored if buffer has previously been initialized!</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use optional stopping or not during training. Could speed up training.
Only works if <code class="docutils literal notranslate"><span class="pre">validation_sims</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, i.e., validation data has been provided.</p>
</dd>
<dt><strong>use_autograph</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use autograph for the backprop step. Could lead to enormous speed-ups but
could also be harder to debug.</p>
</dd>
<dt><strong>validation_sims</strong><span class="classifier">dict or None, optional, default: None</span></dt><dd><p>Simulations used as a “validation set”.
If <code class="docutils literal notranslate"><span class="pre">dict</span></code>, will assume it’s the output of a generative model and try
<code class="docutils literal notranslate"><span class="pre">amortizer.compute_loss(configurator(validation_sims))</span></code>
after each epoch.
If <code class="docutils literal notranslate"><span class="pre">int</span></code>, will assume it’s the number of sims to generate from the generative
model before starting training. Only considered if a generative model has been
provided during initialization.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), no validation set will be used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>losses</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></span></dt><dd><p>A dictionary or a data frame storing the losses across epochs and iterations.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model_args</strong></dt><dd><p>optional kwargs passed to the generative model</p>
</dd>
<dt><strong>val_model_args</strong></dt><dd><p>optional kwargs passed to the generative model for generating validation data. Only useful if
<code class="docutils literal notranslate"><span class="pre">type(validation_sims)</span> <span class="pre">is</span> <span class="pre">int</span></code>.</p>
</dd>
<dt><strong>conf_args</strong></dt><dd><p>optional kwargs passed to the configurator before each backprop (update) step.</p>
</dd>
<dt><strong>val_conf_args</strong></dt><dd><p>optional kwargs passed to the configurator then configuring the validation data.</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional kwargs passed to the amortizer</p>
</dd>
<dt><strong>early_stopping_args:</strong></dt><dd><p>optional kwargs passed to the <cite>EarlyStopper</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.train_rounds">
<span class="sig-name descname"><span class="pre">train_rounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sim_per_round</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_checkpoint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_autograph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_sims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.train_rounds"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.train_rounds" title="Link to this definition">#</a></dt>
<dd><p>Trains an amortizer via round-based learning. In each round, <code class="docutils literal notranslate"><span class="pre">sim_per_round</span></code> data sets
are simulated from the generative model and added to the data sets simulated in previous
round. Then, the networks are trained for <code class="docutils literal notranslate"><span class="pre">epochs</span></code> on the augmented set of data sets.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Training time will increase from round to round, since the number of simulations
increases correspondingly. The final round will then train the networks on <code class="docutils literal notranslate"><span class="pre">rounds</span> <span class="pre">*</span> <span class="pre">sim_per_round</span></code>
data sets, so make sure this number does not eat up all available memory.</p>
</div>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>rounds</strong><span class="classifier">int</span></dt><dd><p>Number of rounds to perform (outer loop)</p>
</dd>
<dt><strong>sim_per_round</strong><span class="classifier">int</span></dt><dd><p>Number of simulations per round</p>
</dd>
<dt><strong>epochs</strong><span class="classifier">int</span></dt><dd><p>Number of epochs (and number of times a checkpoint is stored, inner loop) within a round.</p>
</dd>
<dt><strong>batch_size</strong><span class="classifier">int</span></dt><dd><p>Number of simulations to use at each backpropagation step</p>
</dd>
<dt><strong>save_checkpoint</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>A flag to decide whether to save checkpoints after each epoch,
if a checkpoint_path provided during initialization, otherwise ignored.</p>
</dd>
<dt><strong>optimizer</strong><span class="classifier">tf.keras.optimizer.Optimizer or None</span></dt><dd><p>Optimizer for the neural network training. <code class="docutils literal notranslate"><span class="pre">None</span></code> will result in <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam</span></code>
using a learning rate of 5e-4 and a cosine decay from 5e-4 to 0. A custom optimizer
will override default learning rate and schedule settings.</p>
</dd>
<dt><strong>reuse_optimizer</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>A flag indicating whether the optimizer instance should be treated as persistent or not.
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the optimizer and its states are not stored after training has finished.
Otherwise, the optimizer will be stored as <code class="docutils literal notranslate"><span class="pre">self.optimizer</span></code> and re-used in further training runs.</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, optional, default: False</span></dt><dd><p>Whether to use optional stopping or not during training. Could speed up training.
Only works if <code class="docutils literal notranslate"><span class="pre">validation_sims</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>, i.e., validation data has been provided.
Will be performed within rounds, not between rounds!</p>
</dd>
<dt><strong>use_autograph</strong><span class="classifier">bool, optional, default: True</span></dt><dd><p>Whether to use autograph for the backprop step. Could lead to enormous speed-ups but
could also be harder to debug.</p>
</dd>
<dt><strong>validation_sims</strong><span class="classifier">dict or None, optional, default: None</span></dt><dd><p>Simulations used as a “validation set”.
If <code class="docutils literal notranslate"><span class="pre">dict</span></code>, will assume it’s the output of a generative model and try
<code class="docutils literal notranslate"><span class="pre">amortizer.compute_loss(configurator(validation_sims))</span></code>
after each epoch.
If <code class="docutils literal notranslate"><span class="pre">int</span></code>, will assume it’s the number of sims to generate from the generative
model before starting training. Only considered if a generative model has been
provided during initialization.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), no validation set will be used.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>losses</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">dict</span></code> or <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></span></dt><dd><p>A dictionary or a data frame storing the losses across epochs and iterations</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Other Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>model_args</strong></dt><dd><p>optional kwargs passed to the generative model</p>
</dd>
<dt><strong>val_model_args</strong></dt><dd><p>optional kwargs passed to the generative model for generating validation data. Only useful if
<code class="docutils literal notranslate"><span class="pre">type(validation_sims)</span> <span class="pre">is</span> <span class="pre">int</span></code>.</p>
</dd>
<dt><strong>conf_args</strong></dt><dd><p>optional kwargs passed to the configurator before each backprop (update) step.</p>
</dd>
<dt><strong>val_conf_args</strong></dt><dd><p>optional kwargs passed to the configurator then configuring the validation data.</p>
</dd>
<dt><strong>net_args</strong></dt><dd><p>optional kwargs passed to the amortizer</p>
</dd>
<dt><strong>early_stopping_args</strong></dt><dd><p>optional kwargs passed to the <cite>EarlyStopper</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesflow.trainers.Trainer.mmd_hypothesis_test">
<span class="sig-name descname"><span class="pre">mmd_hypothesis_test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">observed_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reference_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_reference_simulations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_null_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bayesflow/trainers.html#Trainer.mmd_hypothesis_test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bayesflow.trainers.Trainer.mmd_hypothesis_test" title="Link to this definition">#</a></dt>
<dd><p>Performs a sampling-based hypothesis test for detecting Out-Of-Simulation (model misspecification).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>observed_data</strong><span class="classifier">np.ndarray</span></dt><dd><p>Observed data, shape (num_observed, …)</p>
</dd>
<dt><strong>reference_data</strong><span class="classifier">np.ndarray</span></dt><dd><p>Reference data representing samples from the well-specified model, shape (num_reference, …)</p>
</dd>
<dt><strong>num_reference_simulations</strong><span class="classifier">int, default: 1000</span></dt><dd><p>Number of reference simulations (M) simulated from the trainer’s generative model
if no <cite>reference_data</cite> are provided.</p>
</dd>
<dt><strong>num_null_samples</strong><span class="classifier">int, default: 100</span></dt><dd><p>Number of draws from the MMD sampling distribution under the null hypothesis “the trainer’s generative
model is well-specified”</p>
</dd>
<dt><strong>bootstrap</strong><span class="classifier">bool, default: False</span></dt><dd><p>If true, the reference data (see above) are bootstrapped for each sample from the MMD sampling distribution.
If false, a new data set is simulated for computing each draw from the MMD sampling distribution.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>mmd_null_samples</strong><span class="classifier">np.ndarray</span></dt><dd><p>samples from the H0 sampling distribution (“well-specified model”)</p>
</dd>
<dt><strong>mmd_observed</strong><span class="classifier">float</span></dt><dd><p>summary MMD estimate for the observed data sets</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bayesflow.summary_networks.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">bayesflow.summary_networks module</p>
      </div>
    </a>
    <a class="right-next"
       href="bayesflow.configuration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">bayesflow.configuration module</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer"><code class="docutils literal notranslate"><span class="pre">Trainer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.__init__"><code class="docutils literal notranslate"><span class="pre">Trainer.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.diagnose_latent2d"><code class="docutils literal notranslate"><span class="pre">Trainer.diagnose_latent2d()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.diagnose_sbc_histograms"><code class="docutils literal notranslate"><span class="pre">Trainer.diagnose_sbc_histograms()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.load_pretrained_network"><code class="docutils literal notranslate"><span class="pre">Trainer.load_pretrained_network()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_online"><code class="docutils literal notranslate"><span class="pre">Trainer.train_online()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_offline"><code class="docutils literal notranslate"><span class="pre">Trainer.train_offline()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_from_presimulation"><code class="docutils literal notranslate"><span class="pre">Trainer.train_from_presimulation()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_experience_replay"><code class="docutils literal notranslate"><span class="pre">Trainer.train_experience_replay()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.train_rounds"><code class="docutils literal notranslate"><span class="pre">Trainer.train_rounds()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesflow.trainers.Trainer.mmd_hypothesis_test"><code class="docutils literal notranslate"><span class="pre">Trainer.mmd_hypothesis_test()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The BayesFlow authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, BayesFlow authors (lead maintainer: Stefan T. Radev).
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>